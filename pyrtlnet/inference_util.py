import math
from numbers import Number

import numpy as np
from fxpmath import Fxp

"""
``quantized_model_prefix`` is the file name prefix for the quantized saved model file.

The quantized saved model file's suffix is ``.tflite``. The quantized saved model file
is generated by :func:`.quantize_model`, and read by all inference implementations. It
contains the trained model's weights, biases, and quantization metadata.

:func:`.quantize_model` also saves the quantized weights, biases, and quantization
metadata to a NumPy ``.npz`` file, which is easier to work with than the ``.tflite``
file. The ``.npz`` file can be loaded with :class:`.SavedTensors`.
"""
quantized_model_prefix = "quantized"


def normalization_constants(
    s1: np.ndarray, s2: np.ndarray, s3: np.ndarray
) -> tuple[Fxp, np.ndarray]:
    """Normalize multiplier ``m`` to fixed-point ``m0`` and bit-shift ``n``.

    See Section 2.2 in
    `Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference`_.
    The multiplier ``m`` (`Equation 5`) is computed from three scale factors
    ``s1, s2, s3``.

    .. _Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference: https://arxiv.org/pdf/1712.05877.pdf

    This multiplier ``m`` can then be expressed as a pair of ``(m0, n)``, where ``m0``
    is a fixed-point 32-bit multiplier and ``n`` is a bitwise right-shift amount. A
    floating-point multiplication by ``m`` is equivalent to a fixed-point multiplication
    by ``m0``, followed by a bitwise right-shift by ``n``. This fixed-point
    multiplication and bitwise shift are done by :func:`normalize`.

    In other words, ``m == (2 ** -n) * m0``, where ``m0`` must be in the interval
    ``[0.5, 1)``.

    A layer can have per-axis scale factors, so ``s1``, ``s2``, and ``s3`` are vectors
    of scale factors. This function returns a vector of fixed-point ``m0`` values and a
    vector of integer ``n`` values. See `per-axis quantization`_ for details.

    .. _per-axis quantization: https://ai.google.dev/edge/litert/models/quantization_spec#per-axis_vs_per-tensor

    :param s1: Scale factors for the matrix multiplication's left input, which is the
        layer's weight matrix.
    :param s2: Scale factors for the matrix multiplication's right input, which is the
        layer's input matrix.
    :param s3: Scale factors for the matrix multiplication's output, which is the
        layer's output matrix.

    :returns: ``(m0, n)``, where ``m0`` is a fixed-point multiplier in the interval
              ``[0.5, 1)``, ``n`` is a bitwise right-shift amount, and ``m == (2 ** -n)
              * m0``.
    """  # noqa: E501
    # Equation 5.
    m = s1 * s2 / s3

    # Find the smallest value of ``n`` such that ``m0 == m * (2^n)``, where
    # ``m0 >= 0.5`` and ``m0 < 1``.
    m0 = []
    n = []
    for m_in in m:
        for n_out in range(0, 32):
            # Equation 6.
            m0_out = m_in * (2**n_out)
            if m0_out >= 0.5 and m0_out < 1:
                m0.append(m0_out)
                n.append(n_out)
                break
    m0 = np.array(m0)
    n = np.array(n)
    assert len(m0) == len(m)
    assert len(n) == len(m)

    # ``m0`` must be in the interval ``[0.5, 1)`` so it can be unsigned and we only need
    # fractional bits.
    m0 = Fxp(m0, signed=False, n_word=32, n_frac=32)
    return m0, n


class QuantizedLayer:
    """Stores a layer's weights, biases, and quantization metadata.

    This class performs some additional pre-processing on the raw quantization metadata.
    For example, the layer's floating-point scale factor ``m`` is converted to a
    fixed-point scale factor ``m0`` and a bitwise right-shift ``n`` with
    :func:`normalization_constants`.
    """

    scale: np.ndarray
    """The layer output's floating point scale factor ``m``."""

    zero: np.ndarray
    """The layer output's zero point ``z``."""

    m0: np.ndarray
    """The layer's :attr:`scale` can be expressed as a fixed-point scale factor
    :attr:`m0` and a bitwise right-shift :attr:`n`. See :func:`normalization_constants`.
    """

    n: np.ndarray
    """The layer's :attr:`scale` can be expressed as a fixed-point scale factor
    :attr:`m0` and a bitwise right-shift :attr:`n`. See :func:`normalization_constants`.
    """

    weight: np.ndarray
    """The layer's quantized weight."""

    bias: np.ndarray
    """The layer's quantized bias."""

    def __init__(
        self,
        input_scale: np.ndarray,
        weight_scale: np.ndarray,
        weight_zero: np.ndarray,
        output_scale: np.ndarray,
        output_zero: np.ndarray,
        weight: np.ndarray,
        bias: np.ndarray,
    ) -> None:
        """Store a layer's weights, biases, and quantization metadata.

        :param input_scale: Scale factor for the layer's input. The first layer's input
            is special, and must be retrieved from the model separately. The input for
            subsequent layers comes from the preceding layer, so subsequent layer inputs
            use the preceding layer's scale factor. A layer's scale factor can be
            retrieved with :attr:`scale`.
        :param weight_scale: Scale factor for the layer's weight.
        :param weight_zero: Zero point for the layer's weight.
        :param output_scale: Scale factor for the layer's output.
        :param output_zero: Zero point for the layer's output.
        :param weight: The layer's weight.
        :param bias: The layer's bias.
        """
        assert (weight_zero == 0).all()
        self.scale = output_scale
        self.zero = output_zero
        self.m0, self.n = normalization_constants(weight_scale, input_scale, self.scale)
        self.weight = weight
        self.bias = bias


class SavedTensors:
    """
    Loads weights, biases, and quantization metadata saved by :func:`.save_tensors`.
    """

    input_scale: np.ndarray
    """Floating-point scale factor for neural network's input.

    This scale factor can be converted to a fixed-point multiplier by
    :func:`normalization_constants`.
    """

    input_zero: np.ndarray
    """Zero point for neural network's input."""

    layer: list[QuantizedLayer]
    """List of :class:`QuantizedLayer` containing per-layer weights, biases, and
    quantization metadata.
    """

    def __init__(self, quantized_model_name: str) -> None:
        tensors = np.load(quantized_model_name)

        self.input_scale = tensors.get("input.scale")
        self.input_zero = tensors.get("input.zero")

        self.layer = []
        input_scale = self.input_scale
        for layer in range(2):
            current_layer = QuantizedLayer(
                input_scale=input_scale,
                weight_scale=tensors.get(f"layer{layer}.weight.scale"),
                weight_zero=tensors.get(f"layer{layer}.weight.zero"),
                output_scale=tensors.get(f"layer{layer}.output.scale"),
                output_zero=tensors.get(f"layer{layer}.output.zero"),
                weight=tensors.get(f"layer{layer}.weight"),
                bias=tensors.get(f"layer{layer}.bias"),
            )
            self.layer.append(current_layer)
            input_scale = current_layer.scale


def _set_fg(r: int, g: int, b: int) -> str:
    """Return terminal escape codes to set the foreground color to ``{r, g, b}``.

    Requires a terminal that supports 24-bit color.

    :param r: Amount of red, in the range [0, 255].
    :param g: Amount of green, in the range [0, 255].
    :param b: Amount of blue, in the range [0, 255].

    :returns: A terminal escape code to change the current foreground color to
        ``{r, g b}``.

    """
    return f"\033[38;2;{r};{g};{b}m"


def _set_bg(r: int, g: int, b: int) -> str:
    """Return terminal escape codes to set the background color to ``{r, g, b}``.

    Requires a terminal that supports 24-bit color.

    :param r: Amount of red, in the range [0, 255].
    :param g: Amount of green, in the range [0, 255].
    :param b: Amount of blue, in the range [0, 255].

    :returns: A terminal escape code to change the current background color to
        ``{r, g b}``.

    """
    return f"\033[48;2;{r};{g};{b}m"


def _reset() -> str:
    """:returns: Terminal escape codes to reset the foreground and background colors."""
    return "\033[39m\033[49m"


def display_image(image: np.ndarray) -> None:
    """Print an image as ASCII art in a terminal.

    Requires a terminal that supports 24-bit color.

    The image is a 2D array of grayscale pixel values. The pixel values will be
    normalized such that the largest value displays as white, and the smallest value
    displays as black.

    One line of terminal output will contains up to two rows of pixels.

    :param image: Image to display in the terminal.

    """
    num_rows, num_cols = image.shape
    smallest = np.min(image)
    largest = np.max(image)

    def normalize(x: Number) -> Number:
        """Normalize a pixel value to the range [0, 255]."""
        return int(255 * (x + smallest) / (largest - smallest))

    for row in range(0, num_rows, 2):
        line = ""
        for col in range(num_cols):
            # The current row's normalized intensity determines the foreground color.
            fg = normalize(image[row][col])
            bg = 0
            if row + 1 < num_rows:
                # The next row's normalized intensity determines the background color.
                bg = normalize(image[row + 1][col])
            line += f"{_set_fg(fg, fg, fg)}{_set_bg(bg, bg, bg)}▀"
        print(line + _reset())


def _bar(
    index: Number, x: Number, expected: Number, actual: Number, smallest: Number
) -> str:
    """Return a string representing a horizontal bar in a bar chart.

    The bar corresponding to the ``expected`` digit is always colored green. If the
    ``actual`` digit is not the same as the ``expected`` digit, the bar corresponding to
    the ``actual`` digit will be colored red.

    :param index: The digit that the current bar represents.
    :param x: The probability that the input is an image of the digit ``index``.
    :param expected: The expected digit.
    :param actual: The highest-probability digit, according to the model.
    :param smallest: The smallest ``x``-value in the chart. Space will be reserved so it
        can be displayed.
    :returns: A string representing a bar in a bar chart.

    """
    max_bar_length = 10
    bar_length = math.ceil(abs(x) / max_bar_length)
    negative_bar_length = math.ceil(abs(smallest) / max_bar_length)
    if x < 0:
        padding = negative_bar_length - bar_length
    else:
        padding = negative_bar_length

    bar = " " * padding + "▄" * bar_length
    green = _set_fg(0x2C, 0xA0, 0x2C)
    red = _set_fg(0xD6, 0x27, 0x28)
    if expected == actual and actual == index:
        return green + bar + _reset() + " " + str(x) + " (expected, actual)"
    if expected != actual and actual == index:
        return red + bar + _reset() + " " + str(x) + " (actual)"
    if expected != actual and expected == index:
        return green + bar + _reset() + " " + str(x) + " (expected)"
    return bar + " " + str(x)


def display_outputs(output: np.ndarray, expected: int, actual: int) -> None:
    """Display the neural network's output as a bar chart.

    Bars for higher probability digits are displayed before bars for lower probability
    digits.

    The bar corresponding to the ``expected`` digit is always colored green. If the
    ``actual`` digit is not the same as the ``expected`` digit, the bar corresponding to
    the ``actual`` digit will be colored red.

    Sample output with colors omitted::

        7▕           ▄▄▄▄▄▄▄▄▄▄▄ 106 (expected, actual)
        3▕           ▄▄▄▄▄▄ 59
        2▕           ▄▄▄▄▄ 44
        9▕           ▄▄▄▄▄ 44
        8▕           ▄▄▄ 27
        0▕           ▄▄▄ 24
        5▕           ▄ 4
        4▕        ▄▄▄ -30
        1▕       ▄▄▄▄ -31
        6▕ ▄▄▄▄▄▄▄▄▄▄ -100

    In the sample output above, the digit corresponding to each bar is displayed on the
    left, so the digit ``7`` has the highest probability, followed by the digit ``3``.
    The model predicted the digit is a ``7``, and the digit actually was a ``7``
    according to the labeled test data, so the first bar is annotated with "(expected,
    actual)".

    :param output: Neural network's output tensor.
    :param expected: Expected prediction from labeled training data.
    :param actual: Actual prediction from the neural network.
    """
    # If all outputs are positive, start the x-axis at 0, otherwise start the x-axis at
    # the smallest negative number.
    smallest = np.min(output)
    if smallest > 0:
        smallest = 0
    # Enumerate the probabilities. Each enumeration index corresponds to a digit, so
    # `chart_data` is a list of {digit, probability} pairs. Sort by probability,
    # highest to lowest.
    chart_data = sorted(enumerate(output), reverse=True, key=lambda pair: pair[1])
    for index, value in chart_data:
        print(f"{index}▕ {_bar(index, value, expected, actual, smallest)}")
